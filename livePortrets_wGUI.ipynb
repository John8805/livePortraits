{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages and functions\n",
    "import cv2, dlib, os\n",
    "import mls as mls\n",
    "import numpy as np\n",
    "import math\n",
    "from utils import getFaceRect, landmarks2numpy, createSubdiv2D, calculateDelaunayTriangles, insertBoundaryPoints\n",
    "from utils import getVideoParameters, warpTriangle, getRigidAlignment\n",
    "from utils import teethMaskCreate, erodeLipMask, getLips, getLipHeight, drawDelaunay\n",
    "from utils import mainWarpField, copyMouth\n",
    "from utils import hallucinateControlPoints, getInterEyeDistance\n",
    "\n",
    "\"\"\" Select the trackbar parameters\"\"\"\n",
    "# images\n",
    "imageArray = ['',        # indicates the first frame of the video\n",
    "              \"Frida1.jpg\", \"Frida2.jpg\",\n",
    "              \"MonaLisa.jpg\", \"Rosalba_Carriera.jpg\",\n",
    "              \"IanGillan.jpg\", \"AfghanGirl.jpg\"]\n",
    "imageIndex = 0 \n",
    "maximageIndex = len(imageArray) -1\n",
    "imageTrackbarName = \"Images: \\n 0: First frame \\n 1: Frida Kahlo #1 \\n 2: Frida Kahlo #2 \\n 3: Mona Lisa \\n 4: Rosalba Carriera \\n 5: Ian Gillan \\n 6: Afghan Girl\"\n",
    "\n",
    "# video\n",
    "videoArray = [\"anger.avi\", \"smile.avi\", \"teeth_smile.avi\", \"surprise.avi\"]\n",
    "videoIndex = 0\n",
    "maxvideoIndex = len(videoArray) -1\n",
    "videoTrackbarName = \"Videos: \\n 0: Anger \\n 1: Smile \\n 2: Smile with teeth \\n 3: Surprise\"\n",
    "\n",
    "# state of the process\n",
    "OnOFF = 0\n",
    "maxOnOFF = 1\n",
    "processTrackbarName = \"Process: \\n 0 - Still image \\n 1 - Animate\"\n",
    "\n",
    "\n",
    "\"\"\" Get face and landmark detectors\"\"\"\n",
    "faceDetector = dlib.get_frontal_face_detector()\n",
    "PREDICTOR_PATH = \"../common/shape_predictor_68_face_landmarks.dat\"  # Landmark model location\n",
    "landmarkDetector = dlib.shape_predictor(PREDICTOR_PATH)  \n",
    "\n",
    "\"\"\" Function for control window \"\"\"\n",
    "# Bug: multi-line titles of the OpenCV trackbars don't work. So instead the correspondence \n",
    "# between indices and images is listed on an additional image in control window\n",
    "controlWindowName = 'Control'\n",
    "def createControlWindow(controlWindowName, imageTrackbarName, videoTrackbarName, processTrackbarName):\n",
    "    # create a window\n",
    "    cv2.namedWindow(controlWindowName)\n",
    "    # Trackbar to select the image\n",
    "    cv2.createTrackbar( \"Images\", controlWindowName, imageIndex, maximageIndex,  updateImageFN)\n",
    "    # Trackbar to select the video\n",
    "    cv2.createTrackbar( \"Videos\", controlWindowName, videoIndex, maxvideoIndex,  updateVideoFN)\n",
    "    # Trackbar to start/stop the processing\n",
    "    cv2.createTrackbar(\"Process\", controlWindowName, OnOFF, maxOnOFF, runProcess)\n",
    "    \n",
    "    # Write the text with parameters\n",
    "    position = (30, 30)\n",
    "    font_scale = 0.75\n",
    "    color = (255, 255, 255)\n",
    "    thickness = 2\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    line_type = cv2.LINE_AA\n",
    "    text_size, _ = cv2.getTextSize(imageTrackbarName, font, font_scale, thickness)\n",
    "    line_height = text_size[1] + 5\n",
    "\n",
    "    numLines = len(imageTrackbarName.split(\"\\n\")) + \\\n",
    "               len(videoTrackbarName.split(\"\\n\")) + \\\n",
    "               len(processTrackbarName.split(\"\\n\")) + 2\n",
    "    aux_height = 30 + line_height*numLines\n",
    "    aux_width = 350\n",
    "    aux_im = np.zeros((30 + 18*line_height, 350,3), np.uint8)\n",
    "\n",
    "    # write the text with image names\n",
    "    x, y0 = position # initial postion\n",
    "    for i, line in enumerate(imageTrackbarName.split(\"\\n\")):\n",
    "        y = y0 + i * line_height\n",
    "        cv2.putText(aux_im, line, (x,y), font, \n",
    "                    font_scale, color, thickness, cv2.LINE_AA) \n",
    "    # write the text with video names\n",
    "    y0 = y + 2*line_height # initial postion: x is the same\n",
    "    for i, line in enumerate(videoTrackbarName.split(\"\\n\")):\n",
    "        y = y0 + i * line_height\n",
    "        cv2.putText(aux_im, line, (x,y), font, \n",
    "                    font_scale, color, thickness, cv2.LINE_AA) \n",
    "    # write the text of process\n",
    "    y0 = y + 2*line_height # initial postion: x is the same\n",
    "    for i, line in enumerate(processTrackbarName.split(\"\\n\")):\n",
    "        y = y0 + i * line_height\n",
    "        cv2.putText(aux_im, line, (x,y), font, \n",
    "                    font_scale, color, thickness, cv2.LINE_AA) \n",
    "    cv2.imshow(controlWindowName, aux_im)    \n",
    "\n",
    "\"\"\" Trackbar callback functions \"\"\"     \n",
    "# Function to select the image\n",
    "def updateVideoFN( *args ):\n",
    "    global video_fn, OnOFF\n",
    "    videoIndex = args[0]\n",
    "    video_fn = videoArray[videoIndex]\n",
    "    \n",
    "    # drop OnOFF to zero and update the \"Process trackbar\"\n",
    "    OnOFF = 0\n",
    "    cv2.createTrackbar(\"Process\", controlWindowName, OnOFF, maxOnOFF, runProcess)\n",
    "    \n",
    "    LivePortrets()\n",
    "    pass\n",
    "\n",
    "def updateImageFN( *args ):\n",
    "    # update image filename and drop OnOFF to zero\n",
    "    global im_fn, OnOFF\n",
    "    imageIndex = args[0]\n",
    "    im_fn = imageArray[imageIndex]\n",
    "        \n",
    "    # drop OnOFF to zero and update the \"Process trackbar\"\n",
    "    OnOFF = 0\n",
    "    cv2.createTrackbar(\"Process\", controlWindowName, OnOFF, maxOnOFF, runProcess)\n",
    "    \n",
    "    LivePortrets()\n",
    "    pass\n",
    "\n",
    "# Function to run the process (OnOFF=1) or stop it (OnOFF=0)\n",
    "def runProcess( *args ):\n",
    "    global OnOFF\n",
    "    OnOFF = args[0]\n",
    "    LivePortrets()\n",
    "    pass    \n",
    "\n",
    "\"\"\" A \"gatekeeper\" function \"\"\"\n",
    "def LivePortrets():\n",
    "    # Function that checks the image and video and chooses between  showing the original video, image \n",
    "    # or running the main algorithm\n",
    "    global OnOFF # needed to drop \"Process\" trackbar to 0-state in the end\n",
    "    \n",
    "    # Create a VideoCapture object\n",
    "    cap = cv2.VideoCapture(os.path.join(\"video_recorded\", video_fn))\n",
    "    # Check if camera is opened successfully\n",
    "    if (cap.isOpened() == False): \n",
    "        print(\"Unable to read camera feed\")\n",
    "        \n",
    "    ## if user selected a first frame\n",
    "    if im_fn == '': \n",
    "        # read the first frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            # the orginal video doesn't have information about the frame orientation, so  we need to rotate it\n",
    "            frame = np.rot90(frame, 3).copy()            \n",
    "            scaleY = 600./frame.shape[0]   # scale the frame to have a 600 pixel height\n",
    "            frame = cv2.resize(src=frame, dsize=None, fx=scaleY, fy=scaleY, interpolation=cv2.INTER_LINEAR ) \n",
    "        \n",
    "        # create a window and display  frame\n",
    "        cv2.namedWindow('Live Portrets', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow(\"Live Portrets\", frame)\n",
    "        \n",
    "        # show the whole video frame-by-frame if Process is ON\n",
    "        while(OnOFF == 1):\n",
    "            ret, frame = cap.read()\n",
    "            if ret == True:\n",
    "                # the orginal video doesn't have information about the frame orientation, so  we need to rotate it\n",
    "                frame = np.rot90(frame, 3).copy()\n",
    "                scaleY = 600./frame.shape[0]   # scale the frame to have a 600 pixel height\n",
    "                frame = cv2.resize(src=frame, dsize=None, fx=scaleY, fy=scaleY, interpolation=cv2.INTER_LINEAR ) \n",
    "                cv2.imshow(\"Live Portrets\", frame)\n",
    "                if cv2.waitKey(1) & 0xFF == 1:#10:\n",
    "                    continue\n",
    "            else:\n",
    "                # before breaking the loop drop OnOFF to zero and update the \"Process trackbar\"\n",
    "                OnOFF = 0\n",
    "                cv2.createTrackbar(\"Process\", controlWindowName, OnOFF, maxOnOFF, runProcess)\n",
    "                break                \n",
    "    ## if user selected one of the images           \n",
    "    else:\n",
    "        # read and process the image\n",
    "        im = cv2.imread(os.path.join(\"images_scaled\", im_fn))\n",
    "        \n",
    "        if im is None:\n",
    "            print(\"Unable to read the photo\")\n",
    "        else:\n",
    "            # scale the image to have a 600 pixel height\n",
    "            scaleY = 600./im.shape[0]\n",
    "            im = cv2.resize(src=im, dsize=None, fx=scaleY, fy=scaleY, interpolation=cv2.INTER_LINEAR )  \n",
    "    \n",
    "        # create a window\n",
    "        cv2.namedWindow('Live Portrets', cv2.WINDOW_AUTOSIZE)    \n",
    "        \n",
    "        # show image or run the algorithm\n",
    "        if (OnOFF == 0):\n",
    "            cv2.imshow(\"Live Portrets\", im)\n",
    "        elif (OnOFF == 1):\n",
    "            runLivePortrets(im_fn, im, video_fn, cap)\n",
    "\n",
    "\"\"\" Main algorithm \"\"\"\n",
    "def runLivePortrets(im_fn, im, video_fn, cap):\n",
    "    global OnOFF # needed to drop \"Process\" trackbar to 0-state in the end\n",
    "\n",
    "    ########## Get the parameters and landmarks of the image #########\n",
    "    im_height, im_width, im_channels = im.shape\n",
    "\n",
    "    # detect the face and the landmarks\n",
    "    newRect = getFaceRect(im, faceDetector)\n",
    "    landmarks_im = landmarks2numpy(landmarkDetector(im, newRect))\n",
    "\n",
    "    ###########  Get the parameters of the driving video ##########\n",
    "    # Obtain default resolutions of the frame (system dependent) and convert from float to integer.\n",
    "    (time_video, length_video, fps, frame_width, frame_height) = getVideoParameters(cap)\n",
    "\n",
    "    ############### Create new video ######################\n",
    "    output_fn = im_fn[:-4] + \"_\" + video_fn\n",
    "    out = cv2.VideoWriter(os.path.join(\"video_generated\", output_fn),\n",
    "                          cv2.VideoWriter_fourcc('M','J','P','G'), fps, (im_width, im_height))\n",
    "\n",
    "    ############### Initialize the algorithm parameters #################\n",
    "    frame = [] \n",
    "    tform = [] # similarity transformation that alignes video frame to the input image\n",
    "    srcPoints_frame = []\n",
    "    numCP = 68 # number of control points\n",
    "    newRect_frame = []\n",
    "\n",
    "    # Optical Flow\n",
    "    points=[]\n",
    "    pointsPrev=[] \n",
    "    pointsDetectedCur=[] \n",
    "    pointsDetectedPrev=[]\n",
    "    eyeDistanceNotCalculated = True\n",
    "    eyeDistance = 0\n",
    "    isFirstFrame = True\n",
    "\n",
    "    ############### Go over frames #################\n",
    "    while(True):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret == False:\n",
    "            # before breaking the loop drop OnOFF to zero and update the \"Process trackbar\"\n",
    "            OnOFF = 0\n",
    "            cv2.createTrackbar(\"Process\", controlWindowName, OnOFF, maxOnOFF, runProcess)\n",
    "            break  \n",
    "        else:\n",
    "            # the orginal video doesn't have information about the frame orientation, so  we need to rotate it\n",
    "            frame = np.rot90(frame, 3).copy()\n",
    "\n",
    "            # initialize a new frame for the input image\n",
    "            im_new = im.copy()          \n",
    "\n",
    "            ###############    Similarity alignment of the frame #################\n",
    "            # detect the face (only for the first frame) and landmarks\n",
    "            if isFirstFrame: \n",
    "                newRect_frame = getFaceRect(frame, faceDetector)\n",
    "                landmarks_frame_init = landmarks2numpy(landmarkDetector(frame, newRect_frame))\n",
    "\n",
    "                # compute the similarity transformation in the first frame\n",
    "                tform = getRigidAlignment(landmarks_frame_init, landmarks_im)    \n",
    "            else:\n",
    "                landmarks_frame_init = landmarks2numpy(landmarkDetector(frame, newRect_frame))\n",
    "                if tform == []:\n",
    "                    print(\"ERROR: NO SIMILARITY TRANSFORMATION\")\n",
    "\n",
    "            # Apply similarity transform to the frame\n",
    "            frame_aligned = np.zeros((im_height, im_width, im_channels), dtype=im.dtype)\n",
    "            frame_aligned = cv2.warpAffine(frame, tform, (im_width, im_height))\n",
    "\n",
    "            # Change the landmarks locations\n",
    "            landmarks_frame = np.reshape(landmarks_frame_init, (landmarks_frame_init.shape[0], 1, landmarks_frame_init.shape[1]))\n",
    "            landmarks_frame = cv2.transform(landmarks_frame, tform)\n",
    "            landmarks_frame = np.reshape(landmarks_frame, (landmarks_frame_init.shape[0], landmarks_frame_init.shape[1]))\n",
    "\n",
    "            # hallucinate additional control points\n",
    "            if isFirstFrame: \n",
    "                (subdiv_temp, dt_im, landmarks_frame) = hallucinateControlPoints(landmarks_init = landmarks_frame, \n",
    "                                                                                im_shape = frame_aligned.shape, \n",
    "                                                                                INPUT_DIR=\"\", \n",
    "                                                                                performTriangulation = True)\n",
    "                # number of control points\n",
    "                numCP = landmarks_frame.shape[0]\n",
    "            else:\n",
    "                landmarks_frame = np.concatenate((landmarks_frame, np.zeros((numCP-68,2))), axis=0)\n",
    "\n",
    "            ############### Optical Flow and Stabilization #######################\n",
    "            # Convert to grayscale.\n",
    "            imGray = cv2.cvtColor(frame_aligned, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # prepare data for an optical flow\n",
    "            if (isFirstFrame==True):\n",
    "                [pointsPrev.append((p[0], p[1])) for p in landmarks_frame[68:,:]]\n",
    "                [pointsDetectedPrev.append((p[0], p[1])) for p in landmarks_frame[68:,:]]\n",
    "                imGrayPrev = imGray.copy()\n",
    "\n",
    "            # pointsDetectedCur stores results returned by the facial landmark detector\n",
    "            # points stores the stabilized landmark points\n",
    "            points = []\n",
    "            pointsDetectedCur = []\n",
    "            [points.append((p[0], p[1])) for p in landmarks_frame[68:,:]]\n",
    "            [pointsDetectedCur.append((p[0], p[1])) for p in landmarks_frame[68:,:]]\n",
    "\n",
    "            # Convert to numpy float array\n",
    "            pointsArr = np.array(points, np.float32)\n",
    "            pointsPrevArr = np.array(pointsPrev,np.float32)\n",
    "\n",
    "            # If eye distance is not calculated before\n",
    "            if eyeDistanceNotCalculated:\n",
    "                eyeDistance = getInterEyeDistance(landmarks_frame)\n",
    "                eyeDistanceNotCalculated = False\n",
    "\n",
    "            dotRadius = 3 if (eyeDistance > 100) else 2\n",
    "            sigma = eyeDistance * eyeDistance / 400\n",
    "            s = 2*int(eyeDistance/4)+1\n",
    "\n",
    "            #  Set up optical flow params\n",
    "            lk_params = dict(winSize  = (s, s), maxLevel = 5, criteria = (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS, 20, 0.03))\n",
    "            pointsArr, status, err = cv2.calcOpticalFlowPyrLK(imGrayPrev,imGray,pointsPrevArr,pointsArr,**lk_params)\n",
    "            sigma = 100\n",
    "\n",
    "            # Converting to float and back to list\n",
    "            points = np.array(pointsArr,np.float32).tolist()   \n",
    "\n",
    "            # Facial landmark points are the detected landmark and additional control points are tracked landmarks  \n",
    "            landmarks_frame[68:,:] = pointsArr\n",
    "            landmarks_frame = landmarks_frame.astype(np.int32)\n",
    "\n",
    "            # getting ready for the next frame\n",
    "            imGrayPrev = imGray        \n",
    "            pointsPrev = points\n",
    "            pointsDetectedPrev = pointsDetectedCur\n",
    "\n",
    "            ############### End of Optical Flow and Stabilization #######################\n",
    "\n",
    "            # save information of the first frame for the future\n",
    "            if isFirstFrame: \n",
    "                # hallucinate additional control points for a still image\n",
    "                landmarks_list = landmarks_im.copy().tolist()\n",
    "                for p in landmarks_frame[68:]:\n",
    "                    landmarks_list.append([p[0], p[1]])\n",
    "                srcPoints = np.array(landmarks_list)\n",
    "                srcPoints = insertBoundaryPoints(im_width, im_height, srcPoints) \n",
    "\n",
    "                lip_height = getLipHeight(landmarks_im)            \n",
    "                (_, _, maskInnerLips0, _) = teethMaskCreate(im_height, im_width, srcPoints)    \n",
    "                mouth_area0=maskInnerLips0.sum()/255  \n",
    "\n",
    "                # get source location on the first frame\n",
    "                srcPoints_frame = landmarks_frame.copy()\n",
    "                srcPoints_frame = insertBoundaryPoints(im_width, im_height, srcPoints_frame)  \n",
    "\n",
    "                # Write the original image into the output file\n",
    "                out.write(im_new)                  \n",
    "\n",
    "                # Display the original image           \n",
    "                cv2.imshow('Live Portrets', im_new)\n",
    "                # stop for a short while\n",
    "                if cv2.waitKey(1) & 0xFF == 1:#10:\n",
    "                    continue\n",
    "                # Go out of the loop if OnOFF trackbar was changed to 0\n",
    "                if OnOFF==0:\n",
    "                    break\n",
    "\n",
    "                # no need in additional wraps for the first frame\n",
    "                isFirstFrame = False\n",
    "                continue\n",
    "\n",
    "            ############### Warp field #######################               \n",
    "            dstPoints_frame = landmarks_frame\n",
    "            dstPoints_frame = insertBoundaryPoints(im_width, im_height, dstPoints_frame)\n",
    "\n",
    "            # get the new locations of the control points\n",
    "            dstPoints = dstPoints_frame - srcPoints_frame + srcPoints   \n",
    "\n",
    "            # get a warp field, smoothen it and warp the image\n",
    "            im_new = mainWarpField(im,srcPoints,dstPoints,dt_im)       \n",
    "\n",
    "            ############### Mouth cloning #######################\n",
    "            # get the lips and teeth mask\n",
    "            (maskAllLips, hullOuterLipsIndex, maskInnerLips, hullInnerLipsIndex) = teethMaskCreate(im_height, im_width, dstPoints)\n",
    "            mouth_area = maskInnerLips.sum()/255        \n",
    "\n",
    "            # erode the outer mask based on lipHeight\n",
    "            maskAllLipsEroded = erodeLipMask(maskAllLips, lip_height)\n",
    "            \n",
    "            # smooth the mask of inner region of the mouth\n",
    "            maskInnerLips = cv2.GaussianBlur(np.stack((maskInnerLips,maskInnerLips,maskInnerLips), axis=2),(3,3), 10)\n",
    "\n",
    "            # clone/blend the moth part from 'frame_aligned' if needed (for mouth_area/mouth_area0 > 1)\n",
    "            im_new = copyMouth(mouth_area, mouth_area0,\n",
    "                                landmarks_frame, dstPoints,\n",
    "                                frame_aligned, im_new,\n",
    "                                maskAllLipsEroded, hullOuterLipsIndex, maskInnerLips)           \n",
    "\n",
    "            # Write the frame into the file 'output.avi'\n",
    "            out.write(im_new)\n",
    "\n",
    "            # Display the resulting frame    \n",
    "            cv2.imshow('Live Portrets', im_new)\n",
    "            \n",
    "            # stop for a short while\n",
    "            if cv2.waitKey(1) & 0xFF == 1:#10:\n",
    "                continue\n",
    "                \n",
    "            # Go out of the loop if OnOFF trackbar was changed to 0\n",
    "            if OnOFF==0:\n",
    "                break               \n",
    "    # When everything is done, release the video capture and video write objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "          \n",
    "\n",
    "\"\"\" Main part \"\"\"\n",
    "#define the driving video and the image of the face in neutral pose\n",
    "im_fn = imageArray[imageIndex]\n",
    "video_fn = videoArray[videoIndex]\n",
    "LivePortrets()\n",
    "\n",
    "# create control window with a trackbar\n",
    "createControlWindow(controlWindowName, imageTrackbarName, videoTrackbarName, processTrackbarName)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"newFolder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
